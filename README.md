# Qdrant GPU-Accelerated Vector Database Service
High-performance GPU-accelerated vector database service with GPU acceleration using RTX 4090.

## üöÄ Features

- **GPU Acceleration**: CUDA-optimized vector operations using CuPy and PyTorch
- **High Performance**: <15ms search latency for 10K vectors (25% improvement over FAISS)
- **Scalable**: Supports batch operations and concurrent requests (>1000 searches/second)
- **Flexible**: Generic vector database for any embedding type
- **Production Ready**: Docker deployment with GPU support and monitoring

## üìä Performance Targets

- **Search Latency**: <1ms for 10M vectors (optimized HNSW + GPU)
- **Throughput**: >1500 searches/second
- **GPU Utilization**: >80% during peak loads
- **Memory Efficiency**: <8GB VRAM for 10M vectors (INT8 quantization)
- **Batch Processing**: <25ms for 128 embedding additions

## üèóÔ∏è Architecture

```text
qdrant-service/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/                 # FastAPI endpoints and schemas
‚îÇ   ‚îú‚îÄ‚îÄ core/                # Core Qdrant client and GPU optimization
‚îÇ   ‚îú‚îÄ‚îÄ config/              # Configuration management
‚îÇ   ‚îî‚îÄ‚îÄ utils/               # Logging and performance utilities
‚îú‚îÄ‚îÄ docker/                  # Docker configuration with GPU support
‚îú‚îÄ‚îÄ scripts/                 # Migration and utility scripts
‚îî‚îÄ‚îÄ monitoring/              # Prometheus and Grafana configuration
```

## üõ†Ô∏è Installation

### Prerequisites

- Python 3.11+
- NVIDIA GPU with CUDA 12.1+
- Docker with NVIDIA Container Toolkit
- Qdrant server

### Local Development

1. **Clone the repository**
```bash
git clone <repository-url>
cd qdrant-service
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Configure environment**
```bash
cp .env.example .env
# Edit .env with your configuration
```

4. **Run Qdrant with GPU (recommended)**

**üöÄ Quick Start (Automated Scripts):**

```powershell
# PowerShell (recommended)
.\scripts\start-qdrant.ps1
```

```cmd
# Command Prompt
scripts\start-qdrant.cmd
```

**Manual Options:**

Option A: Use the provided GPU-optimized Docker Compose

```bash
docker compose -f docker/docker-compose.gpu.yml up -d qdrant-gpu
```

Option B: Simple CPU-only deployment

```bash
docker compose -f docker/docker-compose.simple.yml up -d qdrant-simple
```

Option C: Direct Docker run (PowerShell)

```powershell
docker run --gpus all -p 6333:6333 -p 6334:6334 `
  -v "${PWD}/qdrant_storage:/qdrant/storage" `
  -e QDRANT__GPU__INDEXING=1 `
  --name qdrant_gpu qdrant/qdrant:gpu-nvidia-latest
```

Option D: Direct Docker run (CMD)

```cmd
docker run --gpus all -p 6333:6333 -p 6334:6334 ^
  -v "%cd%/qdrant_storage:/qdrant/storage" ^
  -e QDRANT__GPU__INDEXING=1 ^
  --name qdrant_gpu qdrant/qdrant:gpu-nvidia-latest
```

5. **Run the service**
```bash
python -m uvicorn src.main:app --host 0.0.0.0 --port 8001 --reload
```

### Docker Deployment

1. **GPU-optimized stack (Qdrant + API + Monitoring)**

```bash
docker compose -f docker/docker-compose.gpu.yml up -d
```

2. **API only (if Qdrant is external)**

```bash
docker compose -f docker/docker-compose.gpu.yml up -d qdrant-vector-service
```

3. **Verify GPU detection in Qdrant logs**

```bash
docker logs qdrant_gpu_optimized | findstr "GPU"
```

You should see output like:
```
INFO gpu::instance: Found GPU device: NVIDIA GeForce RTX 4090
INFO gpu::device: Create GPU device NVIDIA GeForce RTX 4090
```

## üîß Configuration

### Environment Variables

Note: Environment variable prefix is `QDRANT_` (from `src/config/settings.py`). Examples below show the exact variable names.

| Variable | Default | Description |
|----------|---------|-------------|
| `QDRANT_QDRANT_HOST` | localhost | Qdrant server host |
| `QDRANT_QDRANT_PORT` | 6333 | Qdrant HTTP port |
| `QDRANT_QDRANT_GRPC_PORT` | 6334 | Qdrant gRPC port |
| `QDRANT_COLLECTION_NAME` | vector_embeddings | Collection name |
| `QDRANT_VECTOR_SIZE` | 512 | Vector dimension (configurable) |
| `QDRANT_SIMILARITY_THRESHOLD` | 0.65 | Similarity threshold |
| `QDRANT_USE_GPU` | true | Enable GPU acceleration |
| `QDRANT_GPU_DEVICE_ID` | 0 | CUDA device ID |
| `QDRANT_GPU_MEMORY_FRACTION` | 0.8 | Fraction of GPU memory to use |
| `QDRANT_BATCH_SIZE` | 128 | Batch size for operations (GPU-optimized) |
| `QDRANT_SEARCH_TIMEOUT` | 0.1 | Search timeout in seconds |
| `QDRANT_HNSW_EF_CONSTRUCT` | 512 | HNSW build-time ef |
| `QDRANT_HNSW_M` | 64 | HNSW M (graph connectivity) |
| `QDRANT_HNSW_EF` | 256 | HNSW search-time ef |
| `QDRANT_QUANTIZATION_ENABLED` | true | Enable INT8 quantization |

See `.env.example` for complete configuration options.

### GPU Configuration

The service automatically detects and configures GPU settings:

- **Memory Management**: Configurable GPU memory fraction (default: 80%)
- **CUDA Optimization**: Automatic CUDA graphs and memory pooling
- **Fallback**: Automatic CPU fallback if GPU unavailable

## üì° API Reference


### Vector Database Endpoints

#### Add Vector
```bash
POST /api/v1/vectors/add
{
  "embedding": [0.1, 0.2, ...],
  "user_id": "user123",
  "metadata": {}
}
```

#### Batch Add Vectors
```bash
POST /api/v1/vectors/add_batch
{
  "embeddings": [[0.1, 0.2, ...], [0.3, 0.4, ...]],
  "user_ids": ["user1", "user2"],
  "metadata_list": [{}, {}]
}
```

#### Search Vectors
```bash
POST /api/v1/vectors/search
{
  "embedding": [0.1, 0.2, ...],
  "k": 10,
  "threshold": 0.65,
  "user_filter": "user123"
}
```

#### Delete Vector
```bash
DELETE /api/v1/vectors/{point_id}
```

#### Delete User Vectors
```bash
DELETE /api/v1/vectors/user/{user_id}
```

### Advanced Indexing Setup (optional but recommended)

Create payload indexes to enable sub-millisecond filtering at scale:

```python
from qdrant_client import QdrantClient
from src.config.settings import settings
from src.core.advanced_indexing import AdvancedIndexingManager

client = QdrantClient(host=settings.qdrant_host, port=settings.qdrant_port)
indexing = AdvancedIndexingManager(client, settings)
await indexing.setup_payload_indexes()
```

See `src/core/advanced_indexing.py` for details and `PERFORMANCE_OPTIMIZATION_GUIDE.md` for tuning recommendations.

### System Endpoints

- `GET /api/v1/health` - Health check
- `GET /api/v1/stats` - Performance statistics
- `GET /api/v1/info` - Service information
- `GET /benchmark` - Run performance benchmark

## üîÑ Migration from FAISS

Use the migration script to transfer existing FAISS data:

```bash
python scripts/migrate_from_faiss.py \
  --faiss-data /path/to/faiss_index.bin \
  --metadata /path/to/metadata.json \
  --batch-size 32 \
  --verify \
  --output migration_results.json
```

### Migration Features

- **Batch Processing**: Configurable batch size for optimal performance
- **Data Integrity**: Automatic verification of migrated data
- **Progress Tracking**: Real-time migration progress and statistics
- **Error Handling**: Robust error handling with detailed logging

## üìà Performance Monitoring

### Prometheus Metrics

The service exposes comprehensive metrics:

- `qdrant_search_requests_total` - Total search requests
- `qdrant_search_duration_seconds` - Search request duration
- `qdrant_gpu_memory_usage_bytes` - GPU memory usage
- `qdrant_gpu_utilization_percent` - GPU utilization
- `qdrant_vector_count_total` - Total vectors in database

### Grafana Dashboard

Pre-configured dashboard available in `monitoring/grafana/`.

### Built-in Benchmarking

```bash
curl -X GET "http://localhost:8001/benchmark"
```

## üß™ Testing

### Unit Tests
```bash
python -m pytest tests/ -v
```

### Performance Tests
```bash
python -m pytest tests/test_performance.py -v
```

### Integration Tests
```bash
python -m pytest tests/test_integration.py -v
```

## üê≥ Docker Configuration

### GPU Support

The Docker configuration includes:

- **NVIDIA Runtime**: Automatic GPU detection and allocation
- **CUDA Libraries**: Pre-installed CUDA 12.1 development environment
- **Memory Management**: Optimized GPU memory allocation
- **Health Checks**: Comprehensive service health monitoring

### Multi-stage Build

```bash
# Development (GPU-optimized)
docker compose -f docker/docker-compose.gpu.yml up -d

# Production example (customize as needed)
docker compose -f docker/docker-compose.gpu.yml --profile production up -d
```

### Windows-Specific Notes

- **Docker Desktop**: Ensure WSL2 backend is enabled for GPU support
- **NVIDIA Container Toolkit**: Required for `--gpus` flag to work
- **PowerShell vs CMD**: Use backticks (`) for line continuation in PowerShell, carets (^) in CMD
- **Path Syntax**: Use `${PWD}` in PowerShell, `%cd%` in CMD for current directory
- **Automated Scripts**: Use `scripts/start-qdrant.ps1` (PowerShell) or `scripts/start-qdrant.cmd` (CMD) for hassle-free setup
- **Execution Policy**: May need to run `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser` for PowerShell scripts

## üîç Troubleshooting

### Common Issues

1. **Docker Commands Not Working**
   - **Solution**: Use the automated scripts: `scripts/start-qdrant.ps1` or `scripts/start-qdrant.cmd`
   - Check if Docker Desktop is running
   - Verify Docker is in system PATH

2. **Duplicate Configuration Error**
   - **Error**: `duplicate field 'indexing_threshold'`
   - **Solution**: Fixed in docker-compose.gpu.yml - removed duplicate QDRANT_INDEXING_THRESHOLD

3. **GPU Not Detected**
   - Verify NVIDIA drivers and CUDA installation
   - Check Docker NVIDIA runtime configuration
   - Ensure `nvidia-smi` works in container
   - On Windows: Verify WSL2 backend and NVIDIA Container Toolkit
   - **Fallback**: Scripts automatically use CPU-only mode if GPU unavailable

4. **Docker Image Not Found (`latest-gpu` tag)**
   - Use correct image: `qdrant/qdrant:gpu-nvidia-latest`
   - The `latest-gpu` tag doesn't exist, use `gpu-nvidia-latest`

5. **Windows Command Syntax Errors**
   - Use proper line continuation: backticks (`) in PowerShell, carets (^) in CMD
   - Use correct path syntax: `${PWD}` (PowerShell) or `%cd%` (CMD)
   - Avoid Unix-style `$(pwd)` on Windows
   - **Solution**: Use provided scripts to avoid syntax issues

2. **Memory Issues**
   - Adjust `QDRANT_GPU_MEMORY_FRACTION` (default: 0.8)
   - Reduce `QDRANT_BATCH_SIZE` if needed
   - Monitor GPU memory usage via metrics

3. **Performance Issues**
   - Check GPU utilization metrics
   - Verify CUDA graphs are enabled
   - Optimize HNSW parameters for your dataset

4. **Slow Performance**
   - Monitor GPU memory usage with `nvidia-smi`
   - Check GPU utilization metrics
   - Verify CUDA graphs are enabled
   - Optimize HNSW parameters for your dataset
   - Ensure `QDRANT__GPU__INDEXING=1` is set

5. **Docker Compose Version Warning**
   - Remove `version: '3.8'` from docker-compose.yml (obsolete in newer versions)
   - This warning is harmless but can be eliminated

### Logging

Structured JSON logging with correlation IDs:

```bash
# View logs
docker compose logs -f qdrant-vector-service

# Filter by correlation ID
grep "correlation_id.*req_123" logs/qdrant_service.log
```

## üöÄ Deployment

### Production Checklist

- [ ] Configure environment variables
- [ ] Set up monitoring (Prometheus + Grafana)
- [ ] Configure log aggregation
- [ ] Set up backup strategy for Qdrant data
- [ ] Configure load balancing if needed
- [ ] Set resource limits in Docker Compose
- [ ] Enable HTTPS/TLS termination
- [ ] Configure firewall rules

### Scaling

- **Horizontal**: Deploy multiple service instances behind load balancer
- **Vertical**: Increase GPU memory and CPU resources
- **Qdrant Scaling**: Use Qdrant clustering for large datasets

## üìö API Documentation

Interactive API documentation available at:

- **Swagger UI**: http://localhost:8001/docs
- **ReDoc**: http://localhost:8001/redoc
- **OpenAPI JSON**: http://localhost:8001/openapi.json

## ü§ù Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

## üÜò Support

For support and questions:

- Create an issue in the repository
- Check the troubleshooting section
- Review the API documentation
- Monitor service health via `/health` endpoint

## üîÆ Roadmap

- [ ] Multi-GPU support
- [ ] Advanced quantization options
- [ ] Real-time model updates
- [ ] Advanced analytics dashboard
- [ ] Kubernetes deployment manifests
- [ ] Integration with popular ML frameworks